# -*- coding: utf-8 -*-
"""Test_Final_V3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FYnIzjy2GQRqL4QZZfWLKYxIrvQrWUPE

Introduction to the data set
"""

# download and Java and Spark
! apt-get install openjdk-8-jdk-headless -qq > /dev/null
! wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
! tar xf spark-2.4.5-bin-hadoop2.7.tgz
! pip install -q findspark

# set the environment variables for spark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"

! python -m pip install pyspark

#! apt-get install python-lxml

from pyspark import SparkContext, SQLContext
import pyspark.sql.functions as sqlf
from pyspark.sql.functions import isnan, when, count, col, udf
import pandas as pd
import numpy as np
import re
from datetime import datetime
import xml.etree.ElementTree as ET
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import *
from itertools import chain

sc = SparkContext()
sqlContext = SQLContext(sc)
spark = SparkSession.builder.getOrCreate()

sqlContext.setConf("spark.sql.shuffle.partitions", "600")
sqlContext.setConf("spark.default.parallelism", "600")

!apt-get install p7zip-full
!p7zip -d Posts.7z

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/gdrive/My Drive/Colab Notebooks'

custom_schema = StructType([StructField('ID', IntegerType(), True),
                     StructField('PostTypeID', IntegerType(), True),
                     StructField('CreationDate', DateType(), True),
                     StructField('Title', StringType(), True),
                     StructField('Body', StringType(), True),
                     StructField('Tags', StringType(), True)])

text_files = sc.textFile('/content/gdrive/My Drive/Colab Notebooks/Posts.xml')

text_files.take(5)

text_files = text_files.filter(lambda element: ('<?xml version=' not in element) and (element != '<posts>') and (element != '</posts>'))

text_files.take(5)

row_counter = Row('ID', 'PostTypeID', 'CreationDate', 'Title', 'Body', 'Tags')

def get_info(x):
  root = ET.fromstring(x)

  ID = ''
  PostTypeID = ''
  CreationDate = ''
  Title = ''
  Body = ''
  Tags = ''

  try:
    ID = root.attrib['Id']
    PostTypeID = root.attrib['PostTypeId']
    CreationDate = root.attrib['CreationDate']
    try:
      Title = root.attrib['Title']
    except:
      Title = None
    Body = root.attrib['Body']
    try:
      Tags = root.attrib['Tags']
    except:
      Tags = None
  except:
    pass
  
  return row_counter(ID, PostTypeID, CreationDate, Title, Body, Tags)

data = text_files.map(get_info)

data.take(4)

cols = ['ID', 'PostTypeID', 'CreationDate', 'Title', 'Body', 'Tags']

df_Posts = sqlContext.createDataFrame(data, schema = cols)

df_Posts.show()

df_Posts = df_Posts.dropna(subset = ['Title', 'Tags'])

df_Posts.show()

df_Posts.dtypes

df_Posts = df_Posts.withColumn('ID', df_Posts.ID.cast(IntegerType())).withColumn('PostTypeID', df_Posts.PostTypeID.cast(IntegerType())).withColumn('CreationDate', df_Posts.CreationDate.cast(DateType()))

df_Posts.dtypes

df_Posts.show(5)

df_posts_v1 = df_Posts

"""Data Cleaning"""

df_posts_v1.show(5)

df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '<p>', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '</p>', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '\n', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '<pre', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '</pre>', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '<code>', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '</code>', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '<', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '>', ''))
df_posts_v1 = df_posts_v1.withColumn('Body', sqlf.regexp_replace(df_posts_v1.Body, '/', ''))

df_posts_v1.show(5)

df_posts_v1 = df_posts_v1.withColumn('Tags', sqlf.regexp_replace(df_posts_v1.Tags, '><', ', '))
df_posts_v1 = df_posts_v1.withColumn('Tags', sqlf.regexp_replace(df_posts_v1.Tags, '<', ''))
df_posts_v1 = df_posts_v1.withColumn('Tags', sqlf.regexp_replace(df_posts_v1.Tags, '>', ''))

df_posts_v1.show(5)

def ascii_ignore(x):
  return x.encode('ascii', 'ignore').decode('ascii')

ascii_udf = udf(ascii_ignore)

df_posts_v1 = df_posts_v1.withColumn('Body', ascii_udf('Body'))
df_posts_v1 = df_posts_v1.withColumn('Title', ascii_udf('Title'))

df_posts_v1.show(5)

df_posts_v2 = df_posts_v1
df_posts_v2.show(5)

"""EDA"""

df_posts_v2 = df_posts_v2.withColumn('WordCount_Body', sqlf.size(sqlf.split(sqlf.col('Body'), ' ')))
df_posts_v2 = df_posts_v2.withColumn('WordCount_Title', sqlf.size(sqlf.split(sqlf.col('Title'), ' ')))
df_posts_v2.show(5)

"""# **2. Data Processing**

**FlatMap on the Body Column**
"""

Body = df_posts_v2.select("Body")

Body.show(5)

"""**1. Remove the punctuations and converting into lowercase for easy processing.**"""

from pyspark.sql.functions import col, lower, regexp_replace, split

def clean_text(c):
  c = lower(c)
  c = regexp_replace(c, "^rt ", "")
  c = regexp_replace(c, "(https?\://)\S+", "")
  c = regexp_replace(c, "[^a-zA-Z0-9\\s]", "")
  #c = split(c, "\\s+") tokenization...
  return c

clean_text_df = df_posts_v2.select(clean_text(col("Body")).alias("text"))

clean_text_df.show(5)

"""**2. Tokenize (String -> Array<String>)**"""

from pyspark.ml.feature import Tokenizer

tokenizer = Tokenizer(inputCol="text", outputCol="vector")
vector_df = tokenizer.transform(clean_text_df).select("vector")

vector_df.printSchema()
vector_df.show(10)

"""**3. Remove** **stop words**"""

from pyspark.ml.feature import StopWordsRemover

# Define a list of stop words or use default list
remover = StopWordsRemover()
stopwords = remover.getStopWords()

# Display default list
stopwords[:10]

# Specify input/output columns
remover.setInputCol("vector")
remover.setOutputCol("Body_no_stopw")

# Transform existing dataframe with the StopWordsRemover
Body_no_stopw_df = remover.transform(vector_df).select("Body_no_stopw")

# Display
Body_no_stopw_df.printSchema()
Body_no_stopw_df.show()

"""**4. Tokenizing posts into words**"""

# Import stemmer library
from nltk.stem.porter import *

# Instantiate stemmer object
stemmer = PorterStemmer()

# Quick test of the stemming function
tokens = ["thanks", "its", "proverbially", "unexpected", "running"]
for t in tokens:
  print(stemmer.stem(t))

"""**5. Stemming the data and creating unigram**"""

# Create stemmer python function
def stem(in_vec):
    out_vec = []
    for t in in_vec:
        t_stem = stemmer.stem(t)
        if len(t_stem) > 2:
            out_vec.append(t_stem)       
    return out_vec

# Create user defined function for stemming with return type Array<String>
from pyspark.sql.types import *
stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))

# Create new df with Body containing the stemmed tokens 
Body_stemmed_df = (
    Body_no_stopw_df
        .withColumn("Body_stemmed", stemmer_udf("Body_no_stopw"))
        .select("Body_stemmed")
  )

# Rename df and column for clarity
production_df = Body_stemmed_df.select(col("Body_stemmed").alias("unigrams"))

# Display
production_df.printSchema()
production_df.show()

"""**6. Create bigrams**"""

from pyspark.ml.feature import NGram

# Define NGram transformer
ngram = NGram(n=2, inputCol="unigrams", outputCol="bigrams")

# Create bigram_df as a transform of unigram_df using NGram tranformer
production_df = ngram.transform(production_df)

# Display
production_df.printSchema()
production_df.show()

"""**8. Removing short words (len(word) > n) and empty**"""

from pyspark.sql.functions import col, size

production_df = production_df.where(size(col("bigrams")) >= 2)

# Display
production_df.printSchema()
production_df.show()

"""**9. Save data as Spark table for future analysis**"""

#production_df.write.saveAsTable("faam_dataset_production")



from pyspark import SparkConf, SparkContext
from pyspark.mllib.feature import HashingTF
from pyspark.mllib.feature import IDF

from pyspark.ml.feature import HashingTF, IDF, Tokenizer

tokenizers = Tokenizer(inputCol="text", outputCol="words")
wordsDatas = tokenizers.transform(clean_text_df)

wordsDatas.show()

hashingTFs = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=20)
featurizedDatas = hashingTFs.transform(wordsDatas)
# # alternatively, CountVectorizer can also be used to get term frequency vectors

featurizedDatas.show()

# idfs = IDF(inputCol="rawFeatures", outputCol="features")
# idfModel = idfs.fit(featurizedDatas)
# rescaledData = idfModel.transform(featurizedDatas)

# rescaledData.select("label", "features").show()

# set seed for reproducibility
(trainingData, testData) = featurizedDatas.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)
lrModel = lr.fit(trainingData)
predictions = lrModel.transform(testData)
predictions.filter(predictions['prediction'] == 0) \
    .select("Descript","Category","probability","label","prediction") \
    .orderBy("probability", ascending=False) \
    .show(n = 10, truncate = 30)